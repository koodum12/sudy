(==간단한 개념만 잡는 것이기 때문에 자세한 설명이 부족하다== 예:Dying ReLU의 뜻 X)
(그냥 이런게 있다 정도)

- **여러 뉴런에서 들어온 신호의 세기**를 **특정한 값**으로 바꾸기 위해 **활성화 함수**를 **사용**.

- 활성화 함수는 **신호 세기를 조절**하는데,특히 레이어와 레이어 사이에 있어서 **여러 뉴런에서 특정한 뉴런으로 들어가는 신호를 종합**해서 **하나의 값**으로 바꿔 주는 역할을 한다.
![[activation1.png|457]]
(*위 그림에서 x는 뉴런이고 w는 신호 시그마는 활성화 함수 sigmoid는 뉴런(으로 추측)이다* )
## 시그모이드 함수

^231913

 **시그모이드 함수**
	![[images.png|304]]
	로지스틱 함수:**어떤 생물들이 어떤 식으로 증가**하는지 설명하는 모델.
	 ![[sigmoid.png|319]]
	==**시그모이드 함수**==:여러 뉴런에서 들어온 신호 세기를 모아서 **그 값이 0보다 클수록 1에 가까운 숫자**로 바꿔 주고,반대로 신호 세기가 **0보다 작을수록 0에 가까운 숫자**로 바꾸어주는 특징을 가진 함수.
## 하이퍼볼릭탄젠트 함수
 하이퍼볼릭 탄젠트 함수
	 ![[활성화 함수-20240427045650312.webp|500]]
	 (**파란색:시그노이드 함수** | **빨간색:하이퍼볼릭탄젠트 함수**)
	  하이퍼볼릭탄젠트 함수는 값이 작은 신호를 -1에 가까운 숫자로 바꾸어서 내보낸다.
	 시그모이드 함수를 사용하여 출력값이 0에 가까워지면 신경망이 잘 학습하지 못한다는 한계점이 존재한다.
	 하이퍼볼릭 탄젠트는 0이 아닌 -1의 값을 출력하기 때문에 이 한계를 넘을 수 있다.

## 렐루 함수
 - **==ReLU==**
	- ![[활성화 함수-20240427050735612.webp|306]]
	- '고르게 한다'는 뜻의 Rectified와 '직선으로 이루어진'이라는 뜻의 Linear Unit이 결합.
	- **연산 비용이 적고** 구현이 매우 **간단하다**.
	- **입력 값이 음수** 일 때는 **0으로** 바꾸어서 내보내고, 입력값이 **0보다 클 때**는 **입력받는 값이 출력**되는 모습을 볼 수 있다.
	- 렐루 함수는 최근 **인공 신경망을 학습시킬때** 활성화 함수로 주로 사용됨.
- **==LeakyReLU함수==**
	- ![[활성화 함수-20240427053526992.webp|381]]
	- 입력값이 음수일 경우에 출력값이 0으로 같다는 단점이 있어서 이를 해결하기 위해 Leaky렐루 (Leaky ReLU)함수가 새롭게 개발 되어 사용하는 중.
	- ReLU가 갖는 Dying ReLU을 해결하기 위해 나온 함수이다.
	- **Leaky ReLU 함수**는 전달받은 **신호 세기의 합이 음수**일 경우에** ~~0인 값을 출력하지 않고~~,**미세하게나마 차이가 나는 음수의 값**을 전달한다.
- **PReLU함수**
	- ![[활성화 함수-20240427133016500.webp|378]]
	- Leaky ReLU와 거의 유사하지만 새로운 파라미터a를 추가해 x가 음수인 영역에서도 기울기를 학습한다.
- **ELU**
	- ![[활성화 함수-20240427133146072.webp|303]]
	- ELU는 ReLU의 모든 장점을 포함하며 Dying ReLU 문제를 해결했다
	- 출력 값이 거의 0에 가까우며,일반적인 ReLU와 다르게 exp함수를 계산하는 비용이 발생한다

## 소프트맥스 함수
- 최종 결괏값을 정규화하는데 사용하는 함수
- 인공 신경망의 출력층에 소프트맥스 함수를 사용하면 분류 문제를 해결할 수 있다.
- 출력층에 따라서 분류의 개수 또한 달라진다.
- 소프트맥스 함수는 인공 신경망 모델에서 항상 사용되는 것이 아니라 분류 문제에서 사용되는 함수라고 볼 수 있다.